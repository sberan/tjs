name: Regenerate Benchmarks

on:
  push:
    branches:
      - main
  workflow_dispatch:
    inputs:
      validators:
        description: 'Validators to benchmark (comma-separated: ajv,zod,joi or "all")'
        required: false
        default: 'all'

permissions:
  issues: write

jobs:
  regenerate:
    runs-on: ubuntu-latest

    steps:
      - uses: actions/checkout@v4
        with:
          submodules: true
          fetch-depth: 0

      - name: Use Node.js 22
        uses: actions/setup-node@v4
        with:
          node-version: 22
          cache: 'npm'

      - name: Install dependencies
        run: npm ci

      - name: Build
        run: npm run build

      - name: Create results directory
        run: mkdir -p benchmarks/results

      - name: Run benchmarks
        run: |
          INPUT="${{ github.event.inputs.validators }}"
          VALIDATORS=${INPUT:-all}

          if [ "$VALIDATORS" = "all" ]; then
            VALIDATORS="ajv,zod,joi"
          fi

          IFS=',' read -ra VALS <<< "$VALIDATORS"
          for v in "${VALS[@]}"; do
            v=$(echo "$v" | xargs)  # trim whitespace
            echo "Running benchmark for $v..."
            npx tsx benchmarks/bench.ts -v "$v" --json "benchmarks/results/${v}.json"
          done

      - name: Run compliance tests
        run: npm test -- --reporter=verbose 2>&1 || true

      - name: Check for significant changes
        id: check_changes
        run: |
          # Check if changes are significant (exits 0 if significant, 1 if noise)
          if npx tsx benchmarks/check-significance.ts; then
            echo "significant=true" >> $GITHUB_OUTPUT
          else
            echo "significant=false" >> $GITHUB_OUTPUT
            echo "Changes are within noise threshold, skipping update"
          fi

      - name: Generate benchmark report
        if: steps.check_changes.outputs.significant == 'true'
        id: report
        run: |
          # Generate the report content
          REPORT=$(npx tsx benchmarks/generate-issue-report.ts)

          # Save to file for the next step (avoid shell escaping issues)
          echo "$REPORT" > /tmp/benchmark-report.md

          echo "Report generated successfully"

      - name: Update benchmark issue
        if: steps.check_changes.outputs.significant == 'true'
        env:
          GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        run: |
          ISSUE_TITLE="ðŸ“Š Benchmark Results"

          # Find existing benchmark issue
          ISSUE_NUMBER=$(gh issue list --label "benchmarks" --json number --jq '.[0].number' 2>/dev/null || echo "")

          REPORT=$(cat /tmp/benchmark-report.md)

          if [ -n "$ISSUE_NUMBER" ]; then
            echo "Updating existing issue #$ISSUE_NUMBER"
            gh issue edit "$ISSUE_NUMBER" --body "$REPORT"
          else
            echo "Creating new benchmark issue"
            gh issue create \
              --title "$ISSUE_TITLE" \
              --label "benchmarks" \
              --body "$REPORT"
          fi

      - name: No significant changes
        if: steps.check_changes.outputs.significant != 'true'
        run: echo "No significant benchmark changes detected"
